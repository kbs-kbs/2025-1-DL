|언어|라이브러리|버전|컴포넌트/모듈|클래스|용도|
|---|---|---|---|---|---|
|Python|표준 라이브러리||sys|stdout|표준 출력 함수 사용|
||tensorflow|2.18.0|tensorflow.keras.preprocessing|text|토크나이저 사용|
||tensorflow|2.18.0|tensorflow.keras.preprocessing|sequence|토크나이저 사용|
||matplotlib||matplotlib|pyplot|데이터 시각화|

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Flatten,Embedding
from tensorflow.keras.utils import to_categorical
from numpy import array




자연어 처리 과정    

CountVectorizer    
Tfidvectorizer    
HashingVectorizser    
차이점

## 토큰화
- 토큰화 결과는 리스트로 반환
```
# 단어 빈도수 세기

# 전처리하려는 세 개의 문장을 정합니다.
docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',
       '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',
       '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',
       ]
 
# 토큰화 함수를 이용해 전처리 하는 과정입니다.
token = Tokenizer()            # 토큰화 함수 지정
token.fit_on_texts(docs)       # 토큰화 함수에 문장 적용
 
# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.
# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict 클래스를 사용합니다.
print("\n단어 카운트:\n", token.word_counts) 

# 출력되는 순서는 랜덤입니다. 
print("\n문장 카운트: ", token.document_count)
print("\n각 단어가 몇 개의 문장에 포함되어 있는가:\n", token.word_docs)
print("\n각 단어에 매겨진 인덱스 값:\n",  token.word_index)
```


## 원핫 인코딩
- 단어의 벡터화 방법의 일종
- 단어의 유사도를 표현하지 못하는 단점이 있음

```python
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts([text]) # 문자열로 넘겨주면 한 글자씩 쪼개기 때문에 리스트의 형태로 넘겨줘야합니다.
```
